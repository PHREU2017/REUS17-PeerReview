\section{Related Works}
\begin{comment}

\subsection{Introduction to Bayesian Networks}

Bayesian Networks (BNs) are probabilistic models often used in Machine Learning. BNs can be used to model real-world data through a system of nodes and edges. Each node represents a feature, such as cholesterol levels or age. Directed edges connect parent nodes to child nodes, qualitatively representing conditional relationships. Each node can have multiple parents and children. In order to make certain assumptions of independence, BNs must be directed, acyclic graphs (DAGs). This means that the direct edges cannot make cycles within the network.\cite{Russell1995}

The conditional probability table (CPT) attached to each child node details the quantitative effect of the parents. CPTs are useful for looking up the probability of a feature taking a certain value based on observed features. 

\subsection{Structure Learning}

The structure of a BN is often constructed by a domain expert, leaving just the parameters of the network to be learned from the data. In our research, we are learning both the structure and the parameters of the BN using the data. Structure learning can done using search-and-score techniques, constraint-based methods, or a combination of the two. \cite{Vol2012}

For our purposes, we will focus on search-and-score structure learning. In search-and-score algorithms, many structures are created from the data and scored. The network with the best score is returned as the optimal model for the data.

In this study, we use a Greedy Local Search algorithm called Hill-Climbing (HC). This algorithm starts with an edgeless network. One at a time, a new edge is created and the resulting network is scored. If the score improves, the new edge is kept. This continues until the score no longer improves with new edge additions. At this point, the network is considered to be at the top of a hill.

At this point, the backwards phase begins. This phase works like the forward phase, only with edge deletions rather than edge additions. Once this phase completes, the optimal network is returned. 

The main drawback of greedy search algorithms is the tendency to get stuck at a local maximum rather than continuing on to the global maximum, the truly optimal network. This problem can be minimized by using random restarts throughout the search process to better explore the space. 


Structure scoring metrics include Log Likelihood (LL), Bayesian Information Criterion (BIC), Akaike Information Criterion (AIC), and Likelihood-Equivalence Bayesian Dirichlet (BDe). Each of these are described in detail below.

 Log Likelihood (LL), is the simplest scoring metric; the highest score goes to the structure that fits the data best. The sum is as follows:
 \begin{equation}
    LL(B | D) = \sum_{i=1}^{n}\sum_{j=1}^{q_i}\sum_{k=1}^{r_i}N_{ijk}\log(\frac{N_{ijk}}{N_{ij}})
\end{equation}
 where \textit{n} is number of features, $q_i$ is the total possible configurations of the parents of a particular feature, and $r_i$ is the number of states for a particular feature. $N_{ijk}$ represents the total number of times that a feature takes it's $k^{th}$ value and the the feature's parents are in their $j^{th}$ configuration within the given data.
 
 
 Using LL alone can lead to an excessive number of parameters. This is known as overfitting. An overfitted model is very specific to the data it is trained on and does not generalize well to other data sets. AIC and BIC improve on LL by adding a penalizing term for network complexity \cite{Vol2012}.

AIC was developed by Hirotugu Akaike \cite{Akaike1974} in the early 1970s. The score can be calculated using the following equation:
\begin{equation}
    AIC(B | D) = LL(B | D) - |B|
\end{equation}
where the DAG along which the factorization is made is represented by \textit{B} and the data is represented by \textit{D}. The penalizing term is \begin{math}|B|\end{math}, which represents the network complexity in terms of the number of parameters in \textit{B}.

BIC was created in 1978 by Gideon Schwarz \cite{Schwarz1978} as an alteration on AIC. It can by calculated as:
\begin{equation}
    BIC(B | D) = LL(B | D) - \frac{1}{2}\log(N)|B|
\end{equation}
  The sample size of \textit{B} is represented by \textit{N}. The penalizing factor for BIC is greater than that of AIC.

Unlike BIC and AIC, BDe uses a Bayesian approach to scoring. This method was deveoped by Heckerman, Geiger, and Chickering in 1995 \cite{Heckerman1995}. 


In constraint based structure learning algorithms, the algorithms use conditional independence tests to detect the Markov blankets of the variables, which in turn are used to compute the structure of the Bayesian network. \cite{scutari2017package}  The constraint based learning algorithms that we used are incremental association, Semi-interleaved Hiton PC, and Chow Liu.  Incremental Association uses a two fold plan which starts with a fast forward selection process followed by an effort to eliminate false postives.\cite{scutari2017package} The Semi-interleaved Hiton Pc algorithm uses fast forward selection methods for neighbourhood detection created to eliminate nodes early built on the marginal association. \cite{scutari2017package}.  Where as the Chow Liu algorithm learns the tree structure closest to the true one in the probability table.\cite{scutari2017package} This is done by finding the weight likelihood for each edge, and creating the maximum weight spanning tree.\cite{Murphy}

\end{comment}

\subsection{Previous Models of CARDIA Data}
The development of CAC has been modeled by Dynamic Bayesian Networks (DBNs) using data collected in the Coronary Artery Risk Development in Young Adults (CARDIA) study. This temporal model took into account only non-clinical data to identify how life-style decisions young adults make influence CAC levels later in life \cite{Yang}.

CAC level development has also been predicted from CARDIA's measurements of known risk factors such as age, cholesterol, and BMI. This was done using Statistical Relational Learning (SRL) algorithms, specifically Relational Probability Trees (RPTs) and Relational Functional Gradient Boosting (RFGB). The AUC-ROC for RPT was 0.778 $\pm$ 0.02. For RFGB the AUC-ROC was 0.819 $\pm$ 0.01 \cite{Natarajan2013}. 




\begin{comment}

\subsubsection{Early Prediction of Coronary Artery Calcification Levels Using Statistical Relational Learning}
This study uses two kinds of Statistical Relational Learning (SRL) algorithms for predicting CAC- levels, which are Relational Probability Trees and Relational Functional Gadient Boosting.  These algorithms are applied to the set of CARDIA data, using the measured risk factors to predict the possibility of a patient getting Coronary Heart Disease (CHD) later in life. Being able to predict the likelihood of someone developing CHD, enables  

Previous research with the CARDIA database statistically links behaviors and socioeconomic status with psychosocial vulnerability \cite{ScherwitzL, Scherwitz1992}, physical fitness \cite{Lewis1997, Shishehbor}, and relative risks of heart disease \cite{Karlamangla2005}. 

Bayesian Networks have been used to model the longitudinal links between behavioral and socioeconomic data and Coronary Artery Calcification \cite{Yang}. Machine learning techniques have also been used to predict whether an individual has a rare disease based solely on behavioral data \cite{MacLeod2016}. However, this paper contains the first implementation of a Bayesian Network for modeling solely longitudinal behavioral data.


\subsection{Bayesian Networks - Informative}

\subsubsection{Bayesian Reasoning and Machine Learning}
This textbook gives a very thourough introduction to Bayes Nets and other Machine Learing concepts, written for undergraduate level students. \cite{Barber}
\begin{itemize}
    \item Forward:
    \begin{itemize}
        \item R. R. Bouckaert. Bayesian belief networks: from construction to inference. PhD thesis, University of Utrecht, 1995.
        \item J. Besag and P. Green. Spatial statistics and Bayesian computation. Journal of the Royal Statistical Society, Series B, 55:25–37, 1993.
566
    \end{itemize} 
    \item Backwards:
    \begin{itemize}
        \item  Raghavan, Vasanthan, et al. "Modeling temporal activity patterns in dynamic social networks." IEEE Transactions on Computational Social Systems 1.1 (2014): 89-107.
    \end{itemize}
\end{itemize}

\subsubsection{Where Do the Numbers Come From?}
This article outlines the high-level procedures for creating probabilistic networks, as well as common problems encountered. \cite{Druzdzel}
\begin{itemize}
    \item Forward:
    \begin{itemize}
        \item Norman E. Fenton, Martin Neil, Jose Galan Caballero, "Using Ranked Nodes to Model Qualitative Judgments in Bayesian Networks", Knowledge and Data Engineering IEEE Transactions on, vol. 19, pp. 1420-1432, 2007, ISSN 1041-4347.
        \item Pekka Laitila, Kai Virtanen, "Improving Construction of Conditional Probability Tables for Ranked Nodes in Bayesian Networks", Knowledge and Data Engineering IEEE Transactions on, vol. 28, pp. 1691-1705, 2016, ISSN 1041-4347.
        \item Ye Chen, Divakaran Liginlal, "Bayesian Networks for Knowledge-Based Authentication", Knowledge and Data Engineering IEEE Transactions on, vol. 19, pp. 695-710, 2007, ISSN 1041-4347.
        \item Gao Xiao-guang, Yang Yu, Guo Zhi-gao, Chen Da-qing, "Bayesian approach to learn Bayesian networks using data and constraints", Pattern Recognition (ICPR) 2016 23rd International Conference on, pp. 3667-3672, 2016.
        \item E. Rajabally, P. Sen, S. Whittle, J. Dalton, "Aids to Bayesian belief network construction", Intelligent Systems 2004. Proceedings. 2004 2nd International IEEE Conference, vol. 2, pp. 457-461 Vol.2, 2004.
    \end{itemize} 
    \item Backwards:
    \begin{itemize}
        \item Modeling Coronary Artery Calcification Levels From Behavioral Data in a Clinical Study \cite{Yang}
    \end{itemize}
\end{itemize}

\subsubsection{Learning Bayesian Networks: The Combination of Knowledge and Statistical Data}
This paper outlines the BDe scoring metric for Bayesian Networks. This metric combines event equivalency and parameter modularity to simplify the process of integrating someone's prior knowledge into a BN. \cite{Heckerman}

\begin{itemize}
    \item Forward:
    \begin{itemize}
        \item Cooper, G. & Herskovits, E. (January, 1991). A Bayesian method for the induction of probabilistic networks from data. Technical Report SMI-91-1, Section on Medical Informatics, Stanford University.
        \item Chickering, D. (1995a). A transformational characterization of equivalent Bayesian-network structures. In Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, Montreal, QU, pages 87–98. Morgan Kaufmann.
        \item Geiger, D. & Heckerman, D. (1995). A characterization of the Dirichlet distribution with application to learning Bayesian networks. In Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, Montreal, QU, pages 196–207. Morgan Kaufmann.
    \end{itemize}
    \item Backwards:
    \begin{itemize}
        \item Lauritzen, Steffen L. Graphical models. Vol. 17. Clarendon Press, 1996.
        \item Witten, Ian H., et al. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2016.
    \end{itemize}
\end{itemize}

\subsubsection{Aids to Bayesian Belief Network Construction}
An  article outlining various ways to construct BNs. There were links to some potentially useful software applications as well some detail on how to elicit expert opinions for BNs.\cite{Rajabally}

\begin{itemize}
    \item Forward:
    \begin{itemize}
    \item Rajabally, E., Pratyush Sen, S. Whittle, and J. Dalton. 2017. “Aids to Bayesian Belief Network Construction.” In 2004 2nd International IEEE Conference on “Intelligent Systems”. Proceedings (IEEE Cat. No.04EX791), 2:457–61. IEEE. Accessed May 31. doi:10.1109/IS.2004.1344793.
    \item D. Koller, and A Pfeffer, “Objeef-Oriented Bayesian Netwnks” Proceedings of the 13th Annuol Conference on Uncertainty in Artrficiol Intelligence, Rhode Island, 1997, pp.302-313.
    \item M.J. huzdcl, and L.C. van der Gaag “Building probabilistic networks: where do the numbers come from?” IEEE Trmsocbom on Knowledge ondDarn Engineering, vol. 12, no. 4, pp.481486,2000. \cite{Druzdzel}
\end{itemize}
    \item Backwards:
    \begin{itemize}
        \item Wu, C., Yingzi Lin, and Wen-Jun Zhang. "Human attention modeling in a human-machine interface based on the incorporation of contextual features in a Bayesian network." Systems, Man and Cybernetics, 2005 IEEE International Conference on. Vol. 1. IEEE, 2005.
    \end{itemize}
\end{itemize}

\subsubsection{Bayesian Networks without Tears}
This is an introduction to Bayesian Networks with lots of analogies, written for those with limited probability theory background. It might be useful as a reference for explaining concepts in this paper \cite{AmericanAssociationforArtificialIntelligence.1980}

\begin{itemize}
    \item Forward:
    \begin{itemize}
    \item Henrion, M. 1988. Propagating Uncertainty in Bayesian Networks by Logic Sampling. In Uncertainty in Artificial Intelligence 2, eds. J. Lemmer and L. Kanal, 149–163. Amsterdam: North Holland
    \item Cooper, G. F. 1987. Probabilistic Inference Using Belief Networks is NP-Hard, Technical Report, KSL- 87-27, Medical Computer Science Group, Stanford Univ.
Dean,
\end{itemize}
    \item Backwards:
    \begin{itemize}
    \item Baldi, Pierre, and Søren Brunak. Bioinformatics: the machine learning approach. MIT press, 2001.
    \item Buntine, Wray. "A guide to the literature on learning probabilistic networks from data." IEEE Transactions on knowledge and data engineering 8.2 (1996): 195-210.
\end{itemize}
\end{itemize}

\subsubsection{Machine Learning: A Probabilistic Perspective}
A book that goes in depth on the basic math used in creating Bayesian Networks. \cite{Murphy}
\begin{itemize}
    \item Forward:
    \begin{itemize}
    \item Schapire, R. and Y. Freund (2012). Boosting: Foundations and Algo- rithms. MIT Press.
    \item Quinlan, J. R. (1986). Induction of de-
cision trees. Machine Learning 1, 81–106.
\end{itemize}
    \item Backwards:
    \begin{itemize}
    \item Shalev-Shwartz, Shai, and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
    \item Jordan, Michael I., and Tom M. Mitchell. "Machine learning: Trends, perspectives, and prospects." Science 349.6245 (2015): 255-260.
\end{itemize}
\end{itemize}

\subsubsection{Bayesian Network Classifiers}
This article explains naive Bayes and Bayesian Networks.  It goes to touch on Bayesian classifiers and how they can be used to eliminate bias. \cite{Friedman1997}
\begin{itemize}
    \item Forward:
    \begin{itemize}
    \item Domingos, Pedro, and Michael Pazzani. "On the optimality of the simple Bayesian classifier under zero-one loss." Machine learning 29.2 (1997): 103-130.
    \item Wu, Xindong, et al. "Top 10 algorithms in data mining." Knowledge and information systems 14.1 (2008): 1-37.
\end{itemize}
    \item Backward:
    \begin{itemize}
    \item Buntine, W. (1996). A guide to the literature on learning probabilistic networks from data. IEEE Trans. on Knowledge and Data Engineering, 8, 195–210.
    \item Bouckaert, R. R. (1994). Properties of Bayesian network learning algorithms. In R. López de Mantarás & D. Poole (Eds.), Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (pp. 102–109). San Francisco, CA: Morgan Kaufmann.
\end{itemize}
\end{itemize}



\subsection{Bayesian Networks - Applied}

\subsubsection{Modeling Coronary Artery Calcification Levels From Behavioral Data in a Clinical Study}
Using only socio-demographic & health behavior information from longitudinal EHD, researchers attemped to predict patients' CAC-levels in early to middle adulthood using Dynamic Bayesian Networks. \cite{Yang}


\begin{itemize}
    \item Backwards:
    \begin{itemize}
    \item Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Techniques. MIT Press (2009)
4. Liu, Z., Malone, B.M.
    \item Eaton, D., Murphy, K.: Bayesian structure learning using dynamic programming and MCMC. In: UAI (2007)
3.
\end{itemize}
\end{itemize}

\subsection{Modeling Behavior}

\subsubsection{Association of Neighborhood Socioeconomic Status with Physical Fitness in Healthy Young Adults: the CARDIA Study}
The socioeconomic status of the neighborhood someone lives in has an effect on their physical fitness independent of that individual's socioeconomic status. \cite{Shishehbor}

\begin{itemize}
    \item Forward: 
    \begin{itemize}
       \item Popkin BM, Siega-Riz AM, Haines PS. A comparison of dietary trends among racial and socioeconomic groups in the United States. N Engl J Med. 1996; 335(10):716–720. [PubMed: 8703172] 
       \item Hu FB, Manson JE, Stampfer MJ, et al. Diet, lifestyle, and the risk of type 2 diabetes mellitus in women. N Engl J Med. 2001; 345(11):790–797. [PubMed: 11556298] 
       \item . Berkman LF. Tracking social and biological experiences: the social etiology of cardiovascular disease. Circulation. 2005; 111(23):3022–3024. [PubMed: 15956147] 
       \item  Henderson C, Diez Roux AV, Jacobs DR Jr, et al. Neighbourhood characteristics, individual level socioeconomic factors, and depressive symptoms in young adults: the CARDIA study. J Epidemiol Community Health. 2005; 59(4):322–328. [PubMed: 15767387]
        \item  Link BG, Phelan J. Social conditions as fundamental causes of disease. J Health Soc Behav. 1995 Spec No:80–94.
    \end{itemize}

\end{itemize}

\end{comment}